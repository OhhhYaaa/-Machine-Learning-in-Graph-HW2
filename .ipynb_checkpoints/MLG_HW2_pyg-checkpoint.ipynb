{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLG HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rita/miniconda3/envs/jupyterlab/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, torch, json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch_geometric.utils import accuracy,sparse_mx_to_torch_sparse_tensor\n",
    "import torch_geometric.utils \n",
    "# from models.GCN import GCN\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm, trange\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torchvision import datasets, transforms\n",
    "# import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data, Dataset\n",
    "# test \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content :\n",
      "    1     2     3     4     5     6     7     8     9     10    ...  1424  \\\n",
      "0                                                              ...         \n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1425  1426  1427  1428  1429  1430  1431  1432  1433  \n",
      "0                                                        \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     1     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[2 rows x 1433 columns]\n",
      "test_ :\n",
      "        id    to  from\n",
      "0  E10559  2323  2673\n",
      "1   E4849    81  1634\n",
      "train_ : \n",
      "        id    to  from  label\n",
      "0  E10311  2399  2339      0\n",
      "1  E10255  2397  1144      1\n",
      "edge_index : \n",
      "      to  from\n",
      "0  2397  1144\n",
      "1  2450  1312\n",
      "upload : \n",
      "        id  prob\n",
      "0  E10559   0.5\n",
      "1   E4849   0.5\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "test_ = []\n",
    "train_ = []\n",
    "upload = []\n",
    "edge_index = [] \n",
    "for i in range(3):\n",
    "    os.chdir('/home/rita/111/111-2MLG/HW2/dataset{}'.format(i + 1))\n",
    "    temp = pd.read_csv('./content.csv', header = None, sep = '\\t')\n",
    "    temp.sort_values(by = [0], inplace = True)\n",
    "    temp.set_index([0], inplace = True)\n",
    "    # temp = torch.Tensor(np.array(temp)).to(torch.float32)\n",
    "    # print(temp.shape)\n",
    "    content.append(temp)\n",
    "    test_.append(pd.read_csv('./test.csv'))\n",
    "    temp = pd.read_csv('./train.csv')\n",
    "    # print(temp.shape)\n",
    "    train_.append(temp)\n",
    "    temp = temp[temp.label == 1]\n",
    "    temp = temp[['to', 'from']]\n",
    "    temp = temp.reset_index(drop = True)\n",
    "    # print(temp.shape)\n",
    "    edge_index.append(temp)\n",
    "    upload.append(pd.read_csv('./upload.csv'))\n",
    "print('content :\\n', content[0][:2])\n",
    "print('test_ :\\n', test_[0].head(2))\n",
    "print('train_ : \\n', train_[0].head(2))\n",
    "print('edge_index : \\n', edge_index[0].head(2))\n",
    "print('upload : \\n', upload[0].head(2))\n",
    "os.chdir('/home/rita/111/111-2MLG/HW2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "(2708, 1433)\n",
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "# content preprocessing\n",
    "new_features = []\n",
    "for j in range(len(content)) :\n",
    "    t = torch.Tensor(np.array(content[j]))\n",
    "    features_entropy = []\n",
    "    for i in range(t.shape[1]) :\n",
    "        temp = t.T[i]\n",
    "        t1 = torch.sum(temp == 0) / len(temp)\n",
    "        t2 = torch.sum(temp == 1) / len(temp)\n",
    "        temp = torch.tensor([t1, t2])\n",
    "        temp = entropy(temp)\n",
    "        if (temp == 0) :\n",
    "            temp = 0\n",
    "        else :\n",
    "            temp = 1 / temp\n",
    "        features_entropy.append(temp)\n",
    "    features_entropy = torch.tensor(features_entropy).reshape(1, -1)\n",
    "    t = t * features_entropy\n",
    "    t = t.type(torch.float32)\n",
    "    new_features.append(t)\n",
    "print(new_features[0][:2])\n",
    "print(content[0].shape)\n",
    "print(new_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 8686], edge_label_index=[2, 4324], edge_label=[4324])\n",
      "Data(x=[2708, 1433], edge_index=[2, 8686], edge_label_index=[2, 2172])\n"
     ]
    }
   ],
   "source": [
    "# change to Data.Data\n",
    "\n",
    "# data = {\n",
    "#     'num_features' : content[i].shape[1], \n",
    "#     'num_nodes' : content[i].shape[0], \n",
    "#     'x' : content[i], \n",
    "#     'edge_index' : torch.Tensor(np.array(train_[i].iloc[::, 1:3])).T.type(torch.long), \n",
    "#     'edge_label_index' : torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, 1:3])).T.type(torch.long), \n",
    "#     'edge_label' : torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, -1]))\n",
    "# }\n",
    "\n",
    "train_pyg = {}\n",
    "test_pyg = {}\n",
    "for i in range(3):\n",
    "    x = torch.Tensor(np.array(content[i]))\n",
    "    # x = new_features[i]\n",
    "    train_edge_index = torch.Tensor(np.array(train_[i].iloc[::, 1:3])).T.type(torch.long)\n",
    "    train_edge_label_index = torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, 1:3])).T.type(torch.long)\n",
    "    train_edge_label = torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, -1]))\n",
    "    test_edge_label_index = torch.Tensor(np.array(test_[i].iloc[::, 1:3]).astype(float)).T.type(torch.long)\n",
    "    train_data = Data(x = x, edge_index = train_edge_index, edge_label_index = train_edge_label_index, edge_label = train_edge_label)\n",
    "    test_data = Data(x = x, edge_index = train_edge_index, edge_label_index = test_edge_label_index)\n",
    "\n",
    "    train_pyg[i] = train_data.to(device)\n",
    "    test_pyg[i] = test_data.to(device)\n",
    "print(train_pyg[0])\n",
    "print(test_pyg[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    # T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "    #                   add_negative_train_samples=False),\n",
    "])\n",
    "# CUDA_LAUNCH_BLOCKING=1 \n",
    "for i in range(3) :\n",
    "    train_pyg[i] = transform(train_pyg[i])\n",
    "    test_pyg[i] = transform(test_pyg[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "Num. nodes: 2708 (train=140, val=500, test=1000, other=1068)\n",
      "Num. edges: 5278\n",
      "Num. node features: 1433\n",
      "Num. classes: 7\n",
      "Dataset len.: 1\n",
      "Sum of row values without normalization: tensor([ 9., 23., 19.,  ..., 18., 14., 13.])\n",
      "Sum of row values with normalization: tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000])\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Cora()\n",
      "['y', 'val_mask', 'x', 'test_mask', 'edge_index', 'train_mask']\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(\"/tmp/Cora\", name = \"Cora\")\n",
    "num_nodes = dataset.data.num_nodes \n",
    "# For num. edges see: \n",
    "# - https://github.com/pyg-team/pytorch_geometric/issues/343 \n",
    "# - https://github.com/pyg-team/pytorch_geometric/issues/852 \n",
    "num_edges = dataset.data.num_edges // 2 \n",
    "train_len = dataset[0].train_mask.sum() \n",
    "val_len = dataset[0].val_mask.sum() \n",
    "test_len = dataset[0].test_mask.sum() \n",
    "other_len = num_nodes - train_len - val_len - test_len \n",
    "print(f\"Dataset: {dataset.name}\") \n",
    "print(f\"Num. nodes: {num_nodes} (train={train_len}, val={val_len}, test={test_len}, other={other_len})\") \n",
    "print(f\"Num. edges: {num_edges}\") \n",
    "print(f\"Num. node features: {dataset.num_node_features}\") \n",
    "print(f\"Num. classes: {dataset.num_classes}\") \n",
    "print(f\"Dataset len.: {dataset.len()}\")\n",
    "\n",
    "dataset = Planetoid(\"/tmp/Cora\", name=\"Cora\") \n",
    "print(f\"Sum of row values without normalization: {dataset[0].x.sum(dim=-1)}\") \n",
    " \n",
    "dataset = Planetoid(\"/tmp/Cora\", name=\"Cora\", transform=T.NormalizeFeatures()) \n",
    "print(f\"Sum of row values with normalization: {dataset[0].x.sum(dim=-1)}\")\n",
    "\n",
    "print(dataset[0])\n",
    "print(dataset)\n",
    "print(dataset[0].keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[4488], edge_label_index=[2, 4488]), Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[526], edge_label_index=[2, 526]), Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054]))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "# dataset = Planetoid(path, name='Cora', transform=transform)\n",
    "dataset = Planetoid(\"/tmp/Cora\", name = \"Cora\", transform=transform)\n",
    "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
    "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
    "# each element representing the corresponding split.\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "# print(train_data)\n",
    "# print(val_data)\n",
    "# print(test_data)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.num_features : 1433\n",
      "train_data.x :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "train_data.edge_index :  torch.Size([2, 8976])\n",
      "train_data.edge_index :  tensor([[1145,  434,  505,  ..., 2068, 2370, 1668],\n",
      "        [1358, 2408, 2083,  ...,  861, 2367, 1527]], device='cuda:0')\n",
      "train_data.edge_label_index :  torch.Size([2, 4488])\n",
      "train_data.edge_label_index :  tensor([[1145,  434,  505,  ...,  861, 2367, 1527],\n",
      "        [1358, 2408, 2083,  ..., 2068, 2370, 1668]], device='cuda:0')\n",
      "train_data.edge_label_index.size(1) :  4488\n",
      "train_data.edge_label :  torch.Size([4488])\n",
      "train_data.edge_label :  tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('dataset.num_features :', dataset.num_features)\n",
    "print('train_data.x : ', train_data.x)\n",
    "# 所有的 edge (0, 1)\n",
    "print('train_data.edge_index : ', train_data.edge_index.shape)\n",
    "print('train_data.edge_index : ', train_data.edge_index)\n",
    "# label = 1 的 edge\n",
    "print('train_data.edge_label_index : ', train_data.edge_label_index.shape)\n",
    "print('train_data.edge_label_index : ', train_data.edge_label_index)\n",
    "print('train_data.edge_label_index.size(1) : ', train_data.edge_label_index.size(1))\n",
    "print('train_data.edge_label : ', train_data.edge_label.shape)\n",
    "print('train_data.edge_label : ', train_data.edge_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6930, Val: 0.6784, Test: 0.7131\n",
      "Epoch: 002, Loss: 0.6836, Val: 0.6707, Test: 0.7048\n",
      "Epoch: 003, Loss: 0.6984, Val: 0.6743, Test: 0.7138\n",
      "Epoch: 004, Loss: 0.6781, Val: 0.6873, Test: 0.7301\n",
      "Epoch: 005, Loss: 0.6842, Val: 0.6960, Test: 0.7382\n",
      "Epoch: 006, Loss: 0.6866, Val: 0.6987, Test: 0.7372\n",
      "Epoch: 007, Loss: 0.6862, Val: 0.6933, Test: 0.7302\n",
      "Epoch: 008, Loss: 0.6835, Val: 0.6861, Test: 0.7211\n",
      "Epoch: 009, Loss: 0.6775, Val: 0.6808, Test: 0.7141\n",
      "Epoch: 010, Loss: 0.6697, Val: 0.6774, Test: 0.7091\n",
      "Epoch: 011, Loss: 0.6720, Val: 0.6787, Test: 0.7081\n",
      "Epoch: 012, Loss: 0.6672, Val: 0.6885, Test: 0.7163\n",
      "Epoch: 013, Loss: 0.6582, Val: 0.7139, Test: 0.7386\n",
      "Epoch: 014, Loss: 0.6547, Val: 0.7453, Test: 0.7626\n",
      "Epoch: 015, Loss: 0.6491, Val: 0.7545, Test: 0.7685\n",
      "Epoch: 016, Loss: 0.6364, Val: 0.7496, Test: 0.7625\n",
      "Epoch: 017, Loss: 0.6213, Val: 0.7495, Test: 0.7610\n",
      "Epoch: 018, Loss: 0.6080, Val: 0.7687, Test: 0.7740\n",
      "Epoch: 019, Loss: 0.5902, Val: 0.7931, Test: 0.7891\n",
      "Epoch: 020, Loss: 0.5739, Val: 0.7968, Test: 0.7880\n",
      "Epoch: 021, Loss: 0.5661, Val: 0.7930, Test: 0.7832\n",
      "Epoch: 022, Loss: 0.5548, Val: 0.7836, Test: 0.7750\n",
      "Epoch: 023, Loss: 0.5567, Val: 0.8033, Test: 0.7894\n",
      "Epoch: 024, Loss: 0.5519, Val: 0.8119, Test: 0.7948\n",
      "Epoch: 025, Loss: 0.5539, Val: 0.8223, Test: 0.8058\n",
      "Epoch: 026, Loss: 0.5382, Val: 0.8265, Test: 0.8141\n",
      "Epoch: 027, Loss: 0.5365, Val: 0.8425, Test: 0.8299\n",
      "Epoch: 028, Loss: 0.5158, Val: 0.8534, Test: 0.8418\n",
      "Epoch: 029, Loss: 0.5132, Val: 0.8598, Test: 0.8500\n",
      "Epoch: 030, Loss: 0.5115, Val: 0.8649, Test: 0.8558\n",
      "Epoch: 031, Loss: 0.4991, Val: 0.8632, Test: 0.8559\n",
      "Epoch: 032, Loss: 0.5022, Val: 0.8636, Test: 0.8569\n",
      "Epoch: 033, Loss: 0.4992, Val: 0.8678, Test: 0.8600\n",
      "Epoch: 034, Loss: 0.4928, Val: 0.8753, Test: 0.8656\n",
      "Epoch: 035, Loss: 0.4972, Val: 0.8797, Test: 0.8716\n",
      "Epoch: 036, Loss: 0.4880, Val: 0.8820, Test: 0.8769\n",
      "Epoch: 037, Loss: 0.4945, Val: 0.8853, Test: 0.8831\n",
      "Epoch: 038, Loss: 0.4786, Val: 0.8887, Test: 0.8867\n",
      "Epoch: 039, Loss: 0.4777, Val: 0.8896, Test: 0.8893\n",
      "Epoch: 040, Loss: 0.4803, Val: 0.8903, Test: 0.8899\n",
      "Epoch: 041, Loss: 0.4761, Val: 0.8902, Test: 0.8897\n",
      "Epoch: 042, Loss: 0.4655, Val: 0.8902, Test: 0.8905\n",
      "Epoch: 043, Loss: 0.4772, Val: 0.8891, Test: 0.8911\n",
      "Epoch: 044, Loss: 0.4677, Val: 0.8887, Test: 0.8920\n",
      "Epoch: 045, Loss: 0.4711, Val: 0.8899, Test: 0.8934\n",
      "Epoch: 046, Loss: 0.4688, Val: 0.8921, Test: 0.8957\n",
      "Epoch: 047, Loss: 0.4638, Val: 0.8931, Test: 0.8982\n",
      "Epoch: 048, Loss: 0.4613, Val: 0.8941, Test: 0.9006\n",
      "Epoch: 049, Loss: 0.4648, Val: 0.8945, Test: 0.9024\n",
      "Epoch: 050, Loss: 0.4670, Val: 0.8989, Test: 0.9052\n",
      "Epoch: 051, Loss: 0.4622, Val: 0.9011, Test: 0.9064\n",
      "Epoch: 052, Loss: 0.4596, Val: 0.9001, Test: 0.9074\n",
      "Epoch: 053, Loss: 0.4582, Val: 0.8977, Test: 0.9080\n",
      "Epoch: 054, Loss: 0.4532, Val: 0.8993, Test: 0.9089\n",
      "Epoch: 055, Loss: 0.4552, Val: 0.9026, Test: 0.9097\n",
      "Epoch: 056, Loss: 0.4549, Val: 0.9028, Test: 0.9107\n",
      "Epoch: 057, Loss: 0.4610, Val: 0.9008, Test: 0.9104\n",
      "Epoch: 058, Loss: 0.4566, Val: 0.9007, Test: 0.9104\n",
      "Epoch: 059, Loss: 0.4589, Val: 0.9029, Test: 0.9110\n",
      "Epoch: 060, Loss: 0.4520, Val: 0.9036, Test: 0.9119\n",
      "Epoch: 061, Loss: 0.4527, Val: 0.9031, Test: 0.9120\n",
      "Epoch: 062, Loss: 0.4552, Val: 0.9025, Test: 0.9127\n",
      "Epoch: 063, Loss: 0.4526, Val: 0.9029, Test: 0.9131\n",
      "Epoch: 064, Loss: 0.4523, Val: 0.9046, Test: 0.9132\n",
      "Epoch: 065, Loss: 0.4493, Val: 0.9051, Test: 0.9125\n",
      "Epoch: 066, Loss: 0.4558, Val: 0.9054, Test: 0.9126\n",
      "Epoch: 067, Loss: 0.4588, Val: 0.9044, Test: 0.9124\n",
      "Epoch: 068, Loss: 0.4525, Val: 0.9038, Test: 0.9125\n",
      "Epoch: 069, Loss: 0.4492, Val: 0.9042, Test: 0.9130\n",
      "Epoch: 070, Loss: 0.4555, Val: 0.9053, Test: 0.9129\n",
      "Epoch: 071, Loss: 0.4467, Val: 0.9045, Test: 0.9119\n",
      "Epoch: 072, Loss: 0.4450, Val: 0.9033, Test: 0.9105\n",
      "Epoch: 073, Loss: 0.4527, Val: 0.9033, Test: 0.9116\n",
      "Epoch: 074, Loss: 0.4460, Val: 0.9047, Test: 0.9129\n",
      "Epoch: 075, Loss: 0.4472, Val: 0.9062, Test: 0.9138\n",
      "Epoch: 076, Loss: 0.4510, Val: 0.9068, Test: 0.9144\n",
      "Epoch: 077, Loss: 0.4435, Val: 0.9075, Test: 0.9142\n",
      "Epoch: 078, Loss: 0.4434, Val: 0.9061, Test: 0.9139\n",
      "Epoch: 079, Loss: 0.4431, Val: 0.9062, Test: 0.9142\n",
      "Epoch: 080, Loss: 0.4505, Val: 0.9070, Test: 0.9147\n",
      "Epoch: 081, Loss: 0.4509, Val: 0.9075, Test: 0.9160\n",
      "Epoch: 082, Loss: 0.4432, Val: 0.9068, Test: 0.9162\n",
      "Epoch: 083, Loss: 0.4433, Val: 0.9062, Test: 0.9156\n",
      "Epoch: 084, Loss: 0.4447, Val: 0.9056, Test: 0.9144\n",
      "Epoch: 085, Loss: 0.4502, Val: 0.9061, Test: 0.9154\n",
      "Epoch: 086, Loss: 0.4486, Val: 0.9060, Test: 0.9158\n",
      "Epoch: 087, Loss: 0.4483, Val: 0.9056, Test: 0.9160\n",
      "Epoch: 088, Loss: 0.4483, Val: 0.9065, Test: 0.9161\n",
      "Epoch: 089, Loss: 0.4521, Val: 0.9064, Test: 0.9150\n",
      "Epoch: 090, Loss: 0.4458, Val: 0.9074, Test: 0.9152\n",
      "Epoch: 091, Loss: 0.4477, Val: 0.9099, Test: 0.9180\n",
      "Epoch: 092, Loss: 0.4423, Val: 0.9090, Test: 0.9199\n",
      "Epoch: 093, Loss: 0.4343, Val: 0.9097, Test: 0.9212\n",
      "Epoch: 094, Loss: 0.4451, Val: 0.9131, Test: 0.9213\n",
      "Epoch: 095, Loss: 0.4402, Val: 0.9152, Test: 0.9215\n",
      "Epoch: 096, Loss: 0.4390, Val: 0.9159, Test: 0.9232\n",
      "Epoch: 097, Loss: 0.4395, Val: 0.9157, Test: 0.9246\n",
      "Epoch: 098, Loss: 0.4350, Val: 0.9153, Test: 0.9253\n",
      "Epoch: 099, Loss: 0.4347, Val: 0.9165, Test: 0.9245\n",
      "Epoch: 100, Loss: 0.4398, Val: 0.9183, Test: 0.9242\n",
      "Final Test: 0.9242\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4324"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pyg[0].edge_label_index.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels = 256, out_channels = 128):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = nn.functional.leaky_relu(self.conv1(x, edge_index))\n",
    "        # x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "def fit(model, optim, loss_fn, train_data):\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    # z = model.encode(train_data.x, train_data.edge_label_index)\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    # z = model.encode(data['x'].to(device), data['edge_index'].to(device))\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_label_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=int(train_data.edge_label_index.size(1) ), method='sparse')\n",
    "    \n",
    "    # neg_edge_index = negative_sampling(\n",
    "    #     edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "    #     num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "    # neg_edge_index = negative_sampling(\n",
    "    #     edge_index= data['edge_index'], num_nodes= data['num_nodes'],\n",
    "    #     num_neg_samples=data['edge_label_index'].size(1), method='sparse')\n",
    "\n",
    "    # edge_label_index = torch.cat(\n",
    "    #     [data['edge_label_index'], neg_edge_index],\n",
    "    #     dim=-1,\n",
    "    # )\n",
    "    # edge_label = torch.cat([\n",
    "    #     data['edge_label'],\n",
    "    #     data['edge_label'].new_zeros(neg_edge_index.size(1))\n",
    "    # ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = loss_fn(out, edge_label.to(device))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(data):\n",
    "#     model.eval()\n",
    "#     z = model.encode(data['x'], data['edge_index'])\n",
    "#     out = model.decode(z, data['edge_label_index']).view(-1).sigmoid()\n",
    "#     return roc_auc_score(data['edge_label'].cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "def training_loop(data, n_epochs):\n",
    "    model = Net(in_channels = data.x.shape[1]).to(device)\n",
    "    optim = torch.optim.Adam(params=model.parameters(), lr=1e-2)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    for epoch in trange(1, n_epochs + 1):\n",
    "        loss = fit(\n",
    "            model = model, \n",
    "            optim = optim, \n",
    "            loss_fn = loss_fn, \n",
    "            train_data = data\n",
    "        )\n",
    "        if epoch % 500 == 0 :\n",
    "            print('Epoch : {}, Loss : {}'.format(epoch, loss))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = Net(dataset.num_features, 128, 64).to(device)\n",
    "# model = Net(content[i].shape[1], 128, 64).to(device)\n",
    "# # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# best_val_auc = final_test_auc = 0\n",
    "# for epoch in range(1, 11):\n",
    "#     loss = train(data = data)\n",
    "# #     val_auc = test(val_data)\n",
    "# #     test_auc = test(test_data)\n",
    "# #     if val_auc > best_val_auc:\n",
    "# #         best_val_auc = val_auc\n",
    "# #         final_test_auc = test_auc\n",
    "# #     print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "# #           f'Test: {test_auc:.4f}')\n",
    "#     print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "# print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "# z = model.encode(test_data.x, test_data.edge_index)\n",
    "# final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, test) :\n",
    "    z = model.encode(test.x, test.edge_index)\n",
    "    pred = torch.sigmoid(model.decode(z, test.edge_label_index))\n",
    "    \n",
    "\n",
    "    # test_x = torch.Tensor(np.array(test.iloc[::, 1:])).to(device)\n",
    "    # test_y = loaded_model(test_x)\n",
    "    # test_y = pd.DataFrame(test_y)\n",
    "    # pred = pd.concat([test, test_y], axis = 1)\n",
    "    # pred = pred.drop(['to', 'from'], axis = 1)\n",
    "    # pred.columns = ['id', 'prob']\n",
    "    return pred\n",
    "\n",
    "def predict(test_data, model):\n",
    "    z = model.encode(test_data.x, test_data.edge_index)\n",
    "    test_pred = torch.sigmoid(model.decode(z, test_data.edge_label_index))\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 506/4000 [00:07<00:56, 61.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, Loss : 0.4027149975299835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1011/4000 [00:15<00:43, 69.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000, Loss : 0.39198511838912964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1504/4000 [00:23<00:43, 56.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1500, Loss : 0.3779393434524536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2007/4000 [00:30<00:29, 66.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2000, Loss : 0.3730740547180176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2505/4000 [00:39<00:29, 51.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2500, Loss : 0.3732459545135498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3010/4000 [00:48<00:14, 68.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3000, Loss : 0.3738554120063782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 3515/4000 [00:55<00:06, 75.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3500, Loss : 0.37054145336151123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [01:04<00:00, 61.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4000, Loss : 0.37188956141471863\n",
      "torch.Size([2172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 509/4000 [00:09<01:07, 51.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, Loss : 0.40879493951797485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1009/4000 [00:17<01:02, 47.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000, Loss : 0.38874441385269165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1510/4000 [00:25<00:38, 65.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1500, Loss : 0.3812945783138275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2008/4000 [00:32<00:31, 63.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2000, Loss : 0.37493595480918884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2509/4000 [00:40<00:22, 65.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2500, Loss : 0.36911681294441223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3009/4000 [00:47<00:15, 65.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3000, Loss : 0.3704237639904022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 3513/4000 [00:54<00:06, 71.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3500, Loss : 0.3615877628326416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [01:02<00:00, 63.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4000, Loss : 0.3588546812534332\n",
      "torch.Size([1886])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 511/4000 [00:05<00:37, 92.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500, Loss : 0.4231034815311432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1009/4000 [00:11<00:34, 86.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1000, Loss : 0.40018635988235474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1512/4000 [00:17<00:31, 80.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1500, Loss : 0.3844650983810425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2018/4000 [00:23<00:22, 89.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2000, Loss : 0.38708361983299255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2508/4000 [00:29<00:15, 98.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2500, Loss : 0.37915852665901184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3012/4000 [00:35<00:11, 83.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3000, Loss : 0.3707584738731384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 3512/4000 [00:41<00:06, 79.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3500, Loss : 0.3736434876918793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:46<00:00, 85.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4000, Loss : 0.3696138560771942\n",
      "torch.Size([644])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    # if i == 2 :\n",
    "    #     n_epochs = 1000\n",
    "    # else :\n",
    "    #     n_epochs = 500\n",
    "    n_epochs = 4000\n",
    "    model = training_loop(data = train_pyg[i], n_epochs = n_epochs)\n",
    "    # pred = predict(model, test_pyg[i])\n",
    "\n",
    "    z = model.encode(test_pyg[i].x, test_pyg[i].edge_index)\n",
    "    pred = torch.sigmoid(model.decode(z, test_pyg[i].edge_label_index))\n",
    "    print(pred.shape)\n",
    "    pred = pd.DataFrame(pred.cpu().detach().numpy())\n",
    "    pred = pd.concat([test_[i], pred], axis = 1)\n",
    "    pred = pred.drop(['to', 'from'], axis = 1)\n",
    "    pred.columns = ['id', 'prob']\n",
    "    pred.to_csv('./upload/pred_pyg_neg_256_128_leaky_{}.csv'.format(i + 1))\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999707\n",
      "0.5106318\n",
      "0.57688564\n",
      "0.12153205\n"
     ]
    }
   ],
   "source": [
    "print(pred.prob.max())\n",
    "print(pred.prob.median())\n",
    "print(pred.prob.mean())\n",
    "print(pred.prob.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
