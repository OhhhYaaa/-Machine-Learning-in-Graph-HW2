{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLG HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rita/miniconda3/envs/jupyterlab/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, torch, json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch_geometric.utils import accuracy,sparse_mx_to_torch_sparse_tensor\n",
    "import torch_geometric.utils \n",
    "# from models.GCN import GCN\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm, trange\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torchvision import datasets, transforms\n",
    "# import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data, Dataset\n",
    "# test \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content :\n",
      "    1     2     3     4     5     6     7     8     9     10    ...  1424  \\\n",
      "0                                                              ...         \n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1425  1426  1427  1428  1429  1430  1431  1432  1433  \n",
      "0                                                        \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     1     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[2 rows x 1433 columns]\n",
      "test_ :\n",
      "        id    to  from\n",
      "0  E10559  2323  2673\n",
      "1   E4849    81  1634\n",
      "train_ : \n",
      "        id    to  from  label\n",
      "0  E10311  2399  2339      0\n",
      "1  E10255  2397  1144      1\n",
      "edge_index : \n",
      "      to  from\n",
      "0  2397  1144\n",
      "1  2450  1312\n",
      "upload : \n",
      "        id  prob\n",
      "0  E10559   0.5\n",
      "1   E4849   0.5\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "test_ = []\n",
    "train_ = []\n",
    "upload = []\n",
    "edge_index = [] \n",
    "for i in range(3):\n",
    "    os.chdir('/home/rita/111/111-2MLG/HW2/dataset{}'.format(i + 1))\n",
    "    temp = pd.read_csv('./content.csv', header = None, sep = '\\t')\n",
    "    temp.sort_values(by = [0], inplace = True)\n",
    "    temp.set_index([0], inplace = True)\n",
    "    # temp = torch.Tensor(np.array(temp)).to(torch.float32)\n",
    "    # print(temp.shape)\n",
    "    content.append(temp)\n",
    "    test_.append(pd.read_csv('./test.csv'))\n",
    "    temp = pd.read_csv('./train.csv')\n",
    "    # print(temp.shape)\n",
    "    train_.append(temp)\n",
    "    temp = temp[temp.label == 1]\n",
    "    temp = temp[['to', 'from']]\n",
    "    temp = temp.reset_index(drop = True)\n",
    "    # print(temp.shape)\n",
    "    edge_index.append(temp)\n",
    "    upload.append(pd.read_csv('./upload.csv'))\n",
    "print('content :\\n', content[0][:2])\n",
    "print('test_ :\\n', test_[0].head(2))\n",
    "print('train_ : \\n', train_[0].head(2))\n",
    "print('edge_index : \\n', edge_index[0].head(2))\n",
    "print('upload : \\n', upload[0].head(2))\n",
    "os.chdir('/home/rita/111/111-2MLG/HW2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "(2708, 1433)\n",
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "# content preprocessing\n",
    "new_features = []\n",
    "for j in range(len(content)) :\n",
    "    t = torch.Tensor(np.array(content[j]))\n",
    "    features_entropy = []\n",
    "    for i in range(t.shape[1]) :\n",
    "        temp = t.T[i]\n",
    "        t1 = torch.sum(temp == 0) / len(temp)\n",
    "        t2 = torch.sum(temp == 1) / len(temp)\n",
    "        temp = torch.tensor([t1, t2])\n",
    "        temp = entropy(temp)\n",
    "        if (temp == 0) :\n",
    "            temp = 0\n",
    "        else :\n",
    "            temp = 1 / temp\n",
    "        features_entropy.append(temp)\n",
    "    features_entropy = torch.tensor(features_entropy).reshape(1, -1)\n",
    "    t = t * features_entropy\n",
    "    t = t.type(torch.float32)\n",
    "    new_features.append(t)\n",
    "print(new_features[0][:2])\n",
    "print(content[0].shape)\n",
    "print(new_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 8686], edge_label_index=[2, 4324], edge_label=[4324])\n",
      "Data(x=[2708, 1433], edge_index=[2, 8686], edge_label_index=[2, 2172])\n"
     ]
    }
   ],
   "source": [
    "# change to Data.Data\n",
    "\n",
    "# data = {\n",
    "#     'num_features' : content[i].shape[1], \n",
    "#     'num_nodes' : content[i].shape[0], \n",
    "#     'x' : content[i], \n",
    "#     'edge_index' : torch.Tensor(np.array(train_[i].iloc[::, 1:3])).T.type(torch.long), \n",
    "#     'edge_label_index' : torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, 1:3])).T.type(torch.long), \n",
    "#     'edge_label' : torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, -1]))\n",
    "# }\n",
    "\n",
    "train_pyg = {}\n",
    "test_pyg = {}\n",
    "for i in range(3):\n",
    "    x = torch.Tensor(np.array(content[i]))\n",
    "    # x = new_features[i]\n",
    "    train_edge_index = torch.Tensor(np.array(train_[i].iloc[::, 1:3])).T.type(torch.long)\n",
    "    train_edge_label_index = torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, 1:3])).T.type(torch.long)\n",
    "    train_edge_label = torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, -1]))\n",
    "    test_edge_label_index = torch.Tensor(np.array(test_[i].iloc[::, 1:3]).astype(float)).T.type(torch.long)\n",
    "    train_data = Data(x = x, edge_index = train_edge_index, edge_label_index = train_edge_label_index, edge_label = train_edge_label)\n",
    "    test_data = Data(x = x, edge_index = train_edge_index, edge_label_index = test_edge_label_index)\n",
    "\n",
    "    train_pyg[i] = train_data.to(device)\n",
    "    test_pyg[i] = test_data.to(device)\n",
    "print(train_pyg[0])\n",
    "print(test_pyg[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    # T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "    #                   add_negative_train_samples=False),\n",
    "])\n",
    "# CUDA_LAUNCH_BLOCKING=1 \n",
    "for i in range(3) :\n",
    "    train_pyg[i] = transform(train_pyg[i])\n",
    "    test_pyg[i] = transform(test_pyg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class link_predict(nn.Module) :\n",
    "    def __init__(self, features, emb_dim = 128) :\n",
    "        super(link_predict, self).__init__()\n",
    "        self.features = features\n",
    "        self.edge_index = edge_index\n",
    "        self.emb_dim = emb_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.features.shape[1], self.emb_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(self.emb_dim, self.emb_dim // 2)# , \n",
    "            # nn.ReLU(), \n",
    "            # nn.Linear(self.emb_dim // 2, self.emb_dim // 4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, want_edge) :\n",
    "        z = self.mlp(self.features)\n",
    "        out = []\n",
    "        for i in range(want_edge.shape[0]) :\n",
    "            idx1 = want_edge[i, 0].type(torch.LongTensor)\n",
    "            idx2 = want_edge[i, 1].type(torch.LongTensor)\n",
    "            temp = (z[idx1] * z[idx2]).sum()\n",
    "            # temp = torch.matmul(z[idx1], z[idx2].T)\n",
    "            # temp = temp if temp > 0 else 0\n",
    "            temp = temp - torch.mean(temp)\n",
    "            sig = nn.Sigmoid()\n",
    "            temp = sig(temp)\n",
    "            out.append(temp)\n",
    "        return torch.tensor(out).reshape(-1, 1).squeeze()\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels = 128, out_channels = 64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optim, loss_fn, train_data):\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = loss_fn(out, edge_label.to(device))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "def get_poten_edges(features, edge) :\n",
    "    poten_edges = []\n",
    "    \n",
    "    for i in range(edge.shape[0]) :\n",
    "        idx1 = edge[i, 0].type(torch.LongTensor)\n",
    "        idx2 = edge[i, 1].type(torch.LongTensor)\n",
    "        temp = (features[idx1] - features[idx2]) ** 2\n",
    "        temp = torch.sum((features[idx1] - features[idx2]) ** 2)\n",
    "        # print(temp)\n",
    "        # temp = (features[idx1] == features[idx2]).sum() \n",
    "        poten_edges.append(temp)\n",
    "    temp -= temp.min()        \n",
    "    return torch.tensor(poten_edges)   \n",
    "\n",
    "def training_loop(data, train_data, n_epochs):\n",
    "    model1 = Net(in_channels = data.x.shape[1]).to(device)\n",
    "    optim1 = torch.optim.Adam(params=model1.parameters(), lr=1e-2)\n",
    "    loss_fn1 = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model2 = link_predict(data.x).to(device)\n",
    "    optim2 = torch.optim.Adam(model2.parameters(), lr=1e-4, weight_decay = 1e-5)  \n",
    "    loss_fn2 = nn.MSELoss()\n",
    "\n",
    "    features = data.x.to(torch.float32)\n",
    "    train_x = torch.Tensor(np.array(train_data.iloc[::, [1, 2]])).to(torch.float32)\n",
    "    train_y = torch.Tensor(np.array(train_data.iloc[::, -1])).to(torch.float32)\n",
    "    # train_dataset = Data.TensorDataset(train_x, train_y)\n",
    "    # train_loader = DataLoader(\n",
    "    #     dataset = train_dataset,\n",
    "    #     batch_size = 64,\n",
    "    #     shuffle = True,\n",
    "    #     num_workers = 4\n",
    "    # )\n",
    "    edge = train_x.to(device)\n",
    "    labels = train_y.to(device)\n",
    "\n",
    "    for epoch in trange(1, n_epochs + 1):\n",
    "        # pyg\n",
    "        model1.train()\n",
    "        optim1.zero_grad()\n",
    "        z = model1.encode(data.x, data.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=data.edge_index, num_nodes=data.num_nodes,\n",
    "            num_neg_samples=data.edge_label_index.size(1), method='sparse')\n",
    "        edge_label_index = torch.cat(\n",
    "            [data.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        edge_label = torch.cat([\n",
    "            data.edge_label,\n",
    "            data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        out = model1.decode(z, edge_label_index).view(-1)\n",
    "        loss1 = loss_fn1(out, edge_label.to(device))\n",
    "\n",
    "        # self\n",
    "        poten = get_poten_edges(features, edge).to(device)\n",
    "        outputs = model2(edge).to(device)            \n",
    "        loss2 = 0\n",
    "        for j in range(labels.shape[0]) :\n",
    "            if(labels[j] == 1):\n",
    "                loss2 += torch.exp(-(poten[j]/100**2)) * loss_fn2(outputs[j], labels[j])\n",
    "            else :\n",
    "                loss2 += torch.exp((poten[j]/100**2)) * loss_fn2(outputs[j], labels[j])     \n",
    "        loss2 /= edge.shape[0]\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        loss.requires_grad_()\n",
    "        optim2.zero_grad() \n",
    "        loss.backward() \n",
    "        optim1.step()\n",
    "        optim2.step()\n",
    "        # loop.set_description(f'Epoch[{epoch} / {n_epochs}]')\n",
    "\n",
    "        if epoch % 150 == 0 :\n",
    "            print('Epoch : {}, Loss : {}'.format(epoch, loss))\n",
    "    \n",
    "    return model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 150/3000 [21:26<5:50:17,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 150, Loss : 0.6995754241943359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 300/3000 [39:48<5:37:39,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 300, Loss : 0.685890257358551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 450/3000 [58:23<5:09:31,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 450, Loss : 0.6656571626663208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 600/3000 [1:17:08<4:58:08,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 600, Loss : 0.6531903743743896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 750/3000 [1:35:34<4:41:06,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 750, Loss : 0.657034158706665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 900/3000 [1:56:10<4:13:58,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 900, Loss : 0.6471989750862122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1050/3000 [2:15:46<4:19:08,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1050, Loss : 0.644968569278717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1200/3000 [2:42:54<4:05:59,  8.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1200, Loss : 0.6424113512039185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 1350/3000 [3:04:08<3:24:17,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1350, Loss : 0.6416423320770264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1500/3000 [3:24:36<3:10:01,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1500, Loss : 0.6383979320526123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1650/3000 [3:43:19<2:48:44,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1650, Loss : 0.6415646076202393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1800/3000 [4:01:49<2:27:51,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1800, Loss : 0.6341307163238525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1950/3000 [4:20:08<2:08:31,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1950, Loss : 0.6325489282608032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2100/3000 [4:41:00<2:52:17, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2100, Loss : 0.6363070607185364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 2250/3000 [5:07:18<1:30:11,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2250, Loss : 0.6354429721832275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2400/3000 [5:33:34<1:13:31,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2400, Loss : 0.6304590702056885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 2550/3000 [5:51:48<55:41,  7.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2550, Loss : 0.6309664249420166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 2700/3000 [6:17:55<1:02:46, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2700, Loss : 0.6367632746696472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 2850/3000 [6:37:18<18:17,  7.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2850, Loss : 0.6345641016960144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [6:55:31<00:00,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3000, Loss : 0.6282511949539185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 150/3000 [20:06<4:58:40,  6.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 150, Loss : 0.6903376579284668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 300/3000 [37:56<4:47:57,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 300, Loss : 0.662494421005249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 450/3000 [57:13<6:38:49,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 450, Loss : 0.6510093212127686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 600/3000 [1:15:40<5:58:31,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 600, Loss : 0.6431492567062378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 750/3000 [1:35:16<3:32:17,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 750, Loss : 0.6419737339019775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 813/3000 [1:42:01<4:15:53,  7.02s/it]"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "for i in range(3):\n",
    "    # if i == 2 :\n",
    "    #     n_epochs = 2000\n",
    "    # else :\n",
    "    #     n_epochs = 500\n",
    "    n_epochs = 3000\n",
    "    model1, model2 = training_loop(data = train_pyg[i], train_data = train_[i], n_epochs = n_epochs)\n",
    "    # pred = predict(model, test_pyg[i])\n",
    "\n",
    "    z = model1.encode(test_pyg[i].x, test_pyg[i].edge_index)\n",
    "    pred1 = torch.sigmoid(model1.decode(z, test_pyg[i].edge_label_index)).cpu().detach().numpy().reshape(-1, 1)\n",
    "    # print(type(pred1))\n",
    "    # print(pred1.shape)\n",
    "    # pred1 = pd.DataFrame(pred1)\n",
    "    # print(pred1.shape)\n",
    "    z = model2(test_pyg[i].edge_label_index.T).cpu().detach().numpy().reshape(-1, 1)\n",
    "    # print(type(z))\n",
    "    # print(z.shape)\n",
    "    # z = pd.DataFrame(z)\n",
    "    # print(z.shape)\n",
    "    pred = np.hstack([pred1, z])\n",
    "    pred = np.mean(pred, axis = 1)\n",
    "    # print(pred.shape)\n",
    "    \n",
    "    pred = pd.DataFrame(pred)\n",
    "\n",
    "    # pred = pd.DataFrame(pred.cpu().detach().numpy())\n",
    "    pred = pd.concat([test_[i], pred], axis = 1)\n",
    "    pred = pred.drop(['to', 'from'], axis = 1)\n",
    "    pred.columns = ['id', 'prob']\n",
    "    # print(pred.shape)\n",
    "    pred.to_csv('./upload/pred_pyg_mix_norm_{}.csv'.format(i + 1))\n",
    "\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, model):\n",
    "    z = model.encode(test_data.x, test_data.edge_index)\n",
    "    test_pred = torch.sigmoid(model.decode(z, test_data.edge_label_index))\n",
    "    return test_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
