{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLG HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rita/miniconda3/envs/jupyterlab/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, torch, json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch_geometric.utils import accuracy,sparse_mx_to_torch_sparse_tensor\n",
    "import torch_geometric.utils \n",
    "# from models.GCN import GCN\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm, trange\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torchvision import datasets, transforms\n",
    "# import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data, Dataset\n",
    "# test \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content :\n",
      "    1     2     3     4     5     6     7     8     9     10    ...  1424  \\\n",
      "0                                                              ...         \n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "\n",
      "   1425  1426  1427  1428  1429  1430  1431  1432  1433  \n",
      "0                                                        \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     1     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[2 rows x 1433 columns]\n",
      "test_ :\n",
      "        id    to  from\n",
      "0  E10559  2323  2673\n",
      "1   E4849    81  1634\n",
      "train_ : \n",
      "        id    to  from  label\n",
      "0  E10311  2399  2339      0\n",
      "1  E10255  2397  1144      1\n",
      "edge_index : \n",
      "      to  from\n",
      "0  2397  1144\n",
      "1  2450  1312\n",
      "upload : \n",
      "        id  prob\n",
      "0  E10559   0.5\n",
      "1   E4849   0.5\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "test_ = []\n",
    "train_ = []\n",
    "upload = []\n",
    "edge_index = [] \n",
    "for i in range(3):\n",
    "    os.chdir('/home/rita/111/111-2MLG/HW2/dataset{}'.format(i + 1))\n",
    "    temp = pd.read_csv('./content.csv', header = None, sep = '\\t')\n",
    "    temp.sort_values(by = [0], inplace = True)\n",
    "    temp.set_index([0], inplace = True)\n",
    "    # temp = torch.Tensor(np.array(temp)).to(torch.float32)\n",
    "    # print(temp.shape)\n",
    "    content.append(temp)\n",
    "    test_.append(pd.read_csv('./test.csv'))\n",
    "    temp = pd.read_csv('./train.csv')\n",
    "    # print(temp.shape)\n",
    "    train_.append(temp)\n",
    "    temp = temp[temp.label == 1]\n",
    "    temp = temp[['to', 'from']]\n",
    "    temp = temp.reset_index(drop = True)\n",
    "    # print(temp.shape)\n",
    "    edge_index.append(temp)\n",
    "    upload.append(pd.read_csv('./upload.csv'))\n",
    "print('content :\\n', content[0][:2])\n",
    "print('test_ :\\n', test_[0].head(2))\n",
    "print('train_ : \\n', train_[0].head(2))\n",
    "print('edge_index : \\n', edge_index[0].head(2))\n",
    "print('upload : \\n', upload[0].head(2))\n",
    "os.chdir('/home/rita/111/111-2MLG/HW2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# content preprocessing\n",
    "new_features = []\n",
    "for j in range(len(content)) :\n",
    "    t = torch.Tensor(np.array(content[j]))\n",
    "    features_entropy = []\n",
    "    for i in range(t.shape[1]) :\n",
    "        temp = t.T[i]\n",
    "        t1 = torch.sum(temp == 0) / len(temp)\n",
    "        t2 = torch.sum(temp == 1) / len(temp)\n",
    "        temp = torch.tensor([t1, t2])\n",
    "        temp = entropy(temp)\n",
    "        if (temp == 0) :\n",
    "            temp = 0\n",
    "        else :\n",
    "            temp = 1 / temp\n",
    "        features_entropy.append(temp)\n",
    "    features_entropy = torch.tensor(features_entropy).reshape(1, -1)\n",
    "    t = t * features_entropy\n",
    "    t = t.type(torch.float32)\n",
    "    new_features.append(t)\n",
    "print(new_features[0][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[877, 1703], edge_index=[2, 2572], edge_label_index=[2, 1273], edge_label=[1273])\n",
      "Data(x=[877, 1703], edge_index=[2, 2572], edge_label_index=[2, 644])\n"
     ]
    }
   ],
   "source": [
    "# change to Data.Data\n",
    "\n",
    "# data = {\n",
    "#     'num_features' : content[i].shape[1], \n",
    "#     'num_nodes' : content[i].shape[0], \n",
    "#     'x' : content[i], \n",
    "#     'edge_index' : torch.Tensor(np.array(train_[i].iloc[::, 1:3])).T.type(torch.long), \n",
    "#     'edge_label_index' : torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, 1:3])).T.type(torch.long), \n",
    "#     'edge_label' : torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, -1]))\n",
    "# }\n",
    "\n",
    "train_pyg = {}\n",
    "test_pyg = {}\n",
    "for i in range(3):\n",
    "    x = torch.Tensor(np.array(content[i]))\n",
    "    # x = new_features[i]\n",
    "    train_edge_index = torch.Tensor(np.array(train_[i].iloc[::, 1:3])).T.type(torch.long)\n",
    "    train_edge_label_index = torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, 1:3])).T.type(torch.long)\n",
    "    train_edge_label = torch.Tensor(np.array(train_[i][train_[i].label == 1].iloc[::, -1]))\n",
    "    test_edge_label_index = torch.Tensor(np.array(test_[i].iloc[::, 1:3]).astype(float)).T.type(torch.long)\n",
    "    train_data = Data(x = x, edge_index = train_edge_index, edge_label_index = train_edge_label_index, edge_label = train_edge_label)\n",
    "    test_data = Data(x = x, edge_index = train_edge_index, edge_label_index = test_edge_label_index)\n",
    "\n",
    "    train_pyg[i] = train_data.to(device)\n",
    "    test_pyg[i] = test_data.to(device)\n",
    "print(train_pyg[2])\n",
    "print(test_pyg[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    # T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "    #                   add_negative_train_samples=False),\n",
    "])\n",
    "# CUDA_LAUNCH_BLOCKING=1 \n",
    "for i in range(3) :\n",
    "    train_pyg[i] = transform(train_pyg[i])\n",
    "    test_pyg[i] = transform(test_pyg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cora Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "Num. nodes: 2708 (train=140, val=500, test=1000, other=1068)\n",
      "Num. edges: 5278\n",
      "Num. node features: 1433\n",
      "Num. classes: 7\n",
      "Dataset len.: 1\n",
      "Sum of row values without normalization: tensor([ 9., 23., 19.,  ..., 18., 14., 13.])\n",
      "Sum of row values with normalization: tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000])\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Cora()\n",
      "['edge_index', 'test_mask', 'x', 'train_mask', 'val_mask', 'y']\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(\"/tmp/Cora\", name = \"Cora\")\n",
    "num_nodes = dataset.data.num_nodes \n",
    "# For num. edges see: \n",
    "# - https://github.com/pyg-team/pytorch_geometric/issues/343 \n",
    "# - https://github.com/pyg-team/pytorch_geometric/issues/852 \n",
    "num_edges = dataset.data.num_edges // 2 \n",
    "train_len = dataset[0].train_mask.sum() \n",
    "val_len = dataset[0].val_mask.sum() \n",
    "test_len = dataset[0].test_mask.sum() \n",
    "other_len = num_nodes - train_len - val_len - test_len \n",
    "print(f\"Dataset: {dataset.name}\") \n",
    "print(f\"Num. nodes: {num_nodes} (train={train_len}, val={val_len}, test={test_len}, other={other_len})\") \n",
    "print(f\"Num. edges: {num_edges}\") \n",
    "print(f\"Num. node features: {dataset.num_node_features}\") \n",
    "print(f\"Num. classes: {dataset.num_classes}\") \n",
    "print(f\"Dataset len.: {dataset.len()}\")\n",
    "\n",
    "dataset = Planetoid(\"/tmp/Cora\", name=\"Cora\") \n",
    "print(f\"Sum of row values without normalization: {dataset[0].x.sum(dim=-1)}\") \n",
    " \n",
    "dataset = Planetoid(\"/tmp/Cora\", name=\"Cora\", transform=T.NormalizeFeatures()) \n",
    "print(f\"Sum of row values with normalization: {dataset[0].x.sum(dim=-1)}\")\n",
    "\n",
    "print(dataset[0])\n",
    "print(dataset)\n",
    "print(dataset[0].keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[4488], edge_label_index=[2, 4488]), Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[526], edge_label_index=[2, 526]), Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054]))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "# dataset = Planetoid(path, name='Cora', transform=transform)\n",
    "dataset = Planetoid(\"/tmp/Cora\", name = \"Cora\", transform=transform)\n",
    "# After applying the `RandomLinkSplit` transform, the data is transformed from\n",
    "# a data object to a list of tuples (train_data, val_data, test_data), with\n",
    "# each element representing the corresponding split.\n",
    "train_data, val_data, test_data = dataset[0]\n",
    "\n",
    "# print(train_data)\n",
    "# print(val_data)\n",
    "# print(test_data)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dataset.num_features :', dataset.num_features)\n",
    "print('train_data.x : ', train_data.x)\n",
    "# 所有的 edge (0, 1)\n",
    "print('train_data.edge_index : ', train_data.edge_index.shape)\n",
    "print('train_data.edge_index : ', train_data.edge_index)\n",
    "# label = 1 的 edge\n",
    "print('train_data.edge_label_index : ', train_data.edge_label_index.shape)\n",
    "print('train_data.edge_label_index : ', train_data.edge_label_index)\n",
    "print('train_data.edge_label_index.size(1) : ', train_data.edge_label_index.size(1))\n",
    "print('train_data.edge_label : ', train_data.edge_label.shape)\n",
    "print('train_data.edge_label : ', train_data.edge_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "\n",
    "model = Net(dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels = 128, out_channels = 64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = nn.functional.leaky_relu(self.conv1(x, edge_index))\n",
    "        # x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "def fit(model, optim, loss_fn, train_data):\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    # z = model.encode(data['x'].to(device), data['edge_index'].to(device))\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "    # neg_edge_index = negative_sampling(\n",
    "    #     edge_index= data['edge_index'], num_nodes= data['num_nodes'],\n",
    "    #     num_neg_samples=data['edge_label_index'].size(1), method='sparse')\n",
    "\n",
    "    # edge_label_index = torch.cat(\n",
    "    #     [data['edge_label_index'], neg_edge_index],\n",
    "    #     dim=-1,\n",
    "    # )\n",
    "    # edge_label = torch.cat([\n",
    "    #     data['edge_label'],\n",
    "    #     data['edge_label'].new_zeros(neg_edge_index.size(1))\n",
    "    # ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = loss_fn(out, edge_label.to(device))\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(data):\n",
    "#     model.eval()\n",
    "#     z = model.encode(data['x'], data['edge_index'])\n",
    "#     out = model.decode(z, data['edge_label_index']).view(-1).sigmoid()\n",
    "#     return roc_auc_score(data['edge_label'].cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "def training_loop(data, n_epochs):\n",
    "    model = Net(in_channels = data.x.shape[1]).to(device)\n",
    "    optim = torch.optim.Adam(params=model.parameters(), lr=1e-2)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = fit(\n",
    "            model = model, \n",
    "            optim = optim, \n",
    "            loss_fn = loss_fn, \n",
    "            train_data = data\n",
    "        )\n",
    "        if epoch % 100 == 0 :\n",
    "            print('Epoch : {}, Loss : {}'.format(epoch, loss))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# model = Net(dataset.num_features, 128, 64).to(device)\n",
    "# model = Net(content[i].shape[1], 128, 64).to(device)\n",
    "# # optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# best_val_auc = final_test_auc = 0\n",
    "# for epoch in range(1, 11):\n",
    "#     loss = train(data = data)\n",
    "# #     val_auc = test(val_data)\n",
    "# #     test_auc = test(test_data)\n",
    "# #     if val_auc > best_val_auc:\n",
    "# #         best_val_auc = val_auc\n",
    "# #         final_test_auc = test_auc\n",
    "# #     print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "# #           f'Test: {test_auc:.4f}')\n",
    "#     print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "# print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "# z = model.encode(test_data.x, test_data.edge_index)\n",
    "# final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test) :\n",
    "    z = model.encode(test.x, test.edge_index)\n",
    "    pred = torch.sigmoid(model.decode(z, test.edge_label_index))\n",
    "    \n",
    "\n",
    "    # test_x = torch.Tensor(np.array(test.iloc[::, 1:])).to(device)\n",
    "    # test_y = loaded_model(test_x)\n",
    "    # test_y = pd.DataFrame(test_y)\n",
    "    # pred = pd.concat([test, test_y], axis = 1)\n",
    "    # pred = pred.drop(['to', 'from'], axis = 1)\n",
    "    # pred.columns = ['id', 'prob']\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100, Loss : 0.4607372283935547\n",
      "Epoch : 200, Loss : 0.44116517901420593\n",
      "Epoch : 300, Loss : 0.4264395833015442\n",
      "Epoch : 400, Loss : 0.4205430746078491\n",
      "Epoch : 500, Loss : 0.4126579463481903\n",
      "Epoch : 600, Loss : 0.4061504602432251\n",
      "Epoch : 700, Loss : 0.4038366675376892\n",
      "Epoch : 800, Loss : 0.40121349692344666\n",
      "Epoch : 900, Loss : 0.3962206542491913\n",
      "Epoch : 1000, Loss : 0.39370569586753845\n",
      "Epoch : 1100, Loss : 0.3961440622806549\n",
      "Epoch : 1200, Loss : 0.3943099081516266\n",
      "Epoch : 1300, Loss : 0.3916400372982025\n",
      "Epoch : 1400, Loss : 0.39491578936576843\n",
      "Epoch : 1500, Loss : 0.3881003260612488\n",
      "Epoch : 1600, Loss : 0.38706809282302856\n",
      "Epoch : 1700, Loss : 0.388090044260025\n",
      "Epoch : 1800, Loss : 0.3885413706302643\n",
      "Epoch : 1900, Loss : 0.38455501198768616\n",
      "Epoch : 2000, Loss : 0.38809698820114136\n",
      "Epoch : 2100, Loss : 0.38423606753349304\n",
      "Epoch : 2200, Loss : 0.3866063952445984\n",
      "Epoch : 2300, Loss : 0.3830832242965698\n",
      "Epoch : 2400, Loss : 0.38732248544692993\n",
      "Epoch : 2500, Loss : 0.38253697752952576\n",
      "Epoch : 2600, Loss : 0.38215041160583496\n",
      "Epoch : 2700, Loss : 0.3813563883304596\n",
      "Epoch : 2800, Loss : 0.38655054569244385\n",
      "Epoch : 2900, Loss : 0.37772658467292786\n",
      "Epoch : 3000, Loss : 0.37954097986221313\n",
      "Epoch : 3100, Loss : 0.3818841278553009\n",
      "Epoch : 3200, Loss : 0.3827034533023834\n",
      "Epoch : 3300, Loss : 0.37771427631378174\n",
      "Epoch : 3400, Loss : 0.3781496584415436\n",
      "Epoch : 3500, Loss : 0.38132476806640625\n",
      "Epoch : 3600, Loss : 0.37158480286598206\n",
      "Epoch : 3700, Loss : 0.37306272983551025\n",
      "Epoch : 3800, Loss : 0.37925615906715393\n",
      "Epoch : 3900, Loss : 0.3753841817378998\n",
      "Epoch : 4000, Loss : 0.3730163276195526\n",
      "Epoch : 4100, Loss : 0.3714170455932617\n",
      "Epoch : 4200, Loss : 0.3744441270828247\n",
      "Epoch : 4300, Loss : 0.37235623598098755\n",
      "Epoch : 4400, Loss : 0.3733408749103546\n",
      "Epoch : 4500, Loss : 0.3742355704307556\n",
      "Epoch : 4600, Loss : 0.3722507953643799\n",
      "Epoch : 4700, Loss : 0.37468579411506653\n",
      "Epoch : 4800, Loss : 0.3694087564945221\n",
      "Epoch : 4900, Loss : 0.37445083260536194\n",
      "Epoch : 5000, Loss : 0.37217289209365845\n",
      "torch.Size([2172])\n",
      "Epoch : 100, Loss : 0.45194122195243835\n",
      "Epoch : 200, Loss : 0.42206960916519165\n",
      "Epoch : 300, Loss : 0.4150373339653015\n",
      "Epoch : 400, Loss : 0.4049283266067505\n",
      "Epoch : 500, Loss : 0.4012916386127472\n",
      "Epoch : 600, Loss : 0.40009811520576477\n",
      "Epoch : 700, Loss : 0.39429834485054016\n",
      "Epoch : 800, Loss : 0.40055280923843384\n",
      "Epoch : 900, Loss : 0.38867712020874023\n",
      "Epoch : 1000, Loss : 0.3845577538013458\n",
      "Epoch : 1100, Loss : 0.3820361793041229\n",
      "Epoch : 1200, Loss : 0.3851599395275116\n",
      "Epoch : 1300, Loss : 0.3855941593647003\n",
      "Epoch : 1400, Loss : 0.38095706701278687\n",
      "Epoch : 1500, Loss : 0.38006657361984253\n",
      "Epoch : 1600, Loss : 0.38195163011550903\n",
      "Epoch : 1700, Loss : 0.37706878781318665\n",
      "Epoch : 1800, Loss : 0.37564554810523987\n",
      "Epoch : 1900, Loss : 0.3791750967502594\n",
      "Epoch : 2000, Loss : 0.3786012530326843\n",
      "Epoch : 2100, Loss : 0.3754771053791046\n",
      "Epoch : 2200, Loss : 0.3753697872161865\n",
      "Epoch : 2300, Loss : 0.3744608163833618\n",
      "Epoch : 2400, Loss : 0.3706647455692291\n",
      "Epoch : 2500, Loss : 0.37278494238853455\n",
      "Epoch : 2600, Loss : 0.37365758419036865\n",
      "Epoch : 2700, Loss : 0.3721240162849426\n",
      "Epoch : 2800, Loss : 0.36757487058639526\n",
      "Epoch : 2900, Loss : 0.37319326400756836\n",
      "Epoch : 3000, Loss : 0.3714818060398102\n",
      "Epoch : 3100, Loss : 0.3739030063152313\n",
      "Epoch : 3200, Loss : 0.37248337268829346\n",
      "Epoch : 3300, Loss : 0.3745507299900055\n",
      "Epoch : 3400, Loss : 0.3711402118206024\n",
      "Epoch : 3500, Loss : 0.37323319911956787\n",
      "Epoch : 3600, Loss : 0.3717762529850006\n",
      "Epoch : 3700, Loss : 0.37390220165252686\n",
      "Epoch : 3800, Loss : 0.3721899092197418\n",
      "Epoch : 3900, Loss : 0.3679814636707306\n",
      "Epoch : 4000, Loss : 0.3668334186077118\n",
      "Epoch : 4100, Loss : 0.3689557611942291\n",
      "Epoch : 4200, Loss : 0.3688858151435852\n",
      "Epoch : 4300, Loss : 0.37071362137794495\n",
      "Epoch : 4400, Loss : 0.36497512459754944\n",
      "Epoch : 4500, Loss : 0.37061911821365356\n",
      "Epoch : 4600, Loss : 0.36681973934173584\n",
      "Epoch : 4700, Loss : 0.37130460143089294\n",
      "Epoch : 4800, Loss : 0.3751052916049957\n",
      "Epoch : 4900, Loss : 0.36870089173316956\n",
      "Epoch : 5000, Loss : 0.3612910211086273\n",
      "torch.Size([1886])\n",
      "Epoch : 100, Loss : 0.47561538219451904\n",
      "Epoch : 200, Loss : 0.46283966302871704\n",
      "Epoch : 300, Loss : 0.4448160231113434\n",
      "Epoch : 400, Loss : 0.4278988242149353\n",
      "Epoch : 500, Loss : 0.42984333634376526\n",
      "Epoch : 600, Loss : 0.4061359465122223\n",
      "Epoch : 700, Loss : 0.42766204476356506\n",
      "Epoch : 800, Loss : 0.4133462905883789\n",
      "Epoch : 900, Loss : 0.3994334936141968\n",
      "Epoch : 1000, Loss : 0.40790361166000366\n",
      "Epoch : 1100, Loss : 0.39712968468666077\n",
      "Epoch : 1200, Loss : 0.40415769815444946\n",
      "Epoch : 1300, Loss : 0.41209903359413147\n",
      "Epoch : 1400, Loss : 0.39997637271881104\n",
      "Epoch : 1500, Loss : 0.39226141571998596\n",
      "Epoch : 1600, Loss : 0.3887706995010376\n",
      "Epoch : 1700, Loss : 0.3892271816730499\n",
      "Epoch : 1800, Loss : 0.3873613178730011\n",
      "Epoch : 1900, Loss : 0.3879629969596863\n",
      "Epoch : 2000, Loss : 0.3873862624168396\n",
      "Epoch : 2100, Loss : 0.3945380449295044\n",
      "Epoch : 2200, Loss : 0.3842754364013672\n",
      "Epoch : 2300, Loss : 0.39379069209098816\n",
      "Epoch : 2400, Loss : 0.3845190405845642\n",
      "Epoch : 2500, Loss : 0.39463749527931213\n",
      "Epoch : 2600, Loss : 0.3719567358493805\n",
      "Epoch : 2700, Loss : 0.3811974823474884\n",
      "Epoch : 2800, Loss : 0.37973907589912415\n",
      "Epoch : 2900, Loss : 0.3760380744934082\n",
      "Epoch : 3000, Loss : 0.3951428532600403\n",
      "Epoch : 3100, Loss : 0.38318341970443726\n",
      "Epoch : 3200, Loss : 0.3771803081035614\n",
      "Epoch : 3300, Loss : 0.3909314274787903\n",
      "Epoch : 3400, Loss : 0.3803359270095825\n",
      "Epoch : 3500, Loss : 0.3790813982486725\n",
      "Epoch : 3600, Loss : 0.3765697181224823\n",
      "Epoch : 3700, Loss : 0.38233402371406555\n",
      "Epoch : 3800, Loss : 0.3753497302532196\n",
      "Epoch : 3900, Loss : 0.38114723563194275\n",
      "Epoch : 4000, Loss : 0.3789316713809967\n",
      "Epoch : 4100, Loss : 0.37133195996284485\n",
      "Epoch : 4200, Loss : 0.3856107294559479\n",
      "Epoch : 4300, Loss : 0.3735233247280121\n",
      "Epoch : 4400, Loss : 0.3861144781112671\n",
      "Epoch : 4500, Loss : 0.3674691915512085\n",
      "Epoch : 4600, Loss : 0.37643754482269287\n",
      "Epoch : 4700, Loss : 0.36113208532333374\n",
      "Epoch : 4800, Loss : 0.37119707465171814\n",
      "Epoch : 4900, Loss : 0.37048834562301636\n",
      "Epoch : 5000, Loss : 0.3691179156303406\n",
      "Epoch : 5100, Loss : 0.36211472749710083\n",
      "Epoch : 5200, Loss : 0.36920806765556335\n",
      "Epoch : 5300, Loss : 0.38107970356941223\n",
      "Epoch : 5400, Loss : 0.36920076608657837\n",
      "Epoch : 5500, Loss : 0.3623167872428894\n",
      "Epoch : 5600, Loss : 0.3712189495563507\n",
      "Epoch : 5700, Loss : 0.3745654821395874\n",
      "Epoch : 5800, Loss : 0.3744535744190216\n",
      "Epoch : 5900, Loss : 0.3663322925567627\n",
      "Epoch : 6000, Loss : 0.3682321310043335\n",
      "Epoch : 6100, Loss : 0.36644303798675537\n",
      "Epoch : 6200, Loss : 0.3778478503227234\n",
      "Epoch : 6300, Loss : 0.36194202303886414\n",
      "Epoch : 6400, Loss : 0.37847939133644104\n",
      "Epoch : 6500, Loss : 0.3733363449573517\n",
      "Epoch : 6600, Loss : 0.36880460381507874\n",
      "Epoch : 6700, Loss : 0.370787113904953\n",
      "Epoch : 6800, Loss : 0.37057042121887207\n",
      "Epoch : 6900, Loss : 0.3656877875328064\n",
      "Epoch : 7000, Loss : 0.36736899614334106\n",
      "Epoch : 7100, Loss : 0.376153826713562\n",
      "Epoch : 7200, Loss : 0.35781243443489075\n",
      "Epoch : 7300, Loss : 0.3636454939842224\n",
      "Epoch : 7400, Loss : 0.36526376008987427\n",
      "Epoch : 7500, Loss : 0.37047088146209717\n",
      "Epoch : 7600, Loss : 0.3725724220275879\n",
      "Epoch : 7700, Loss : 0.3711341321468353\n",
      "Epoch : 7800, Loss : 0.3603646457195282\n",
      "Epoch : 7900, Loss : 0.3622114956378937\n",
      "Epoch : 8000, Loss : 0.36252927780151367\n",
      "Epoch : 8100, Loss : 0.36900952458381653\n",
      "Epoch : 8200, Loss : 0.3676721453666687\n",
      "Epoch : 8300, Loss : 0.3678206503391266\n",
      "Epoch : 8400, Loss : 0.37320342659950256\n",
      "Epoch : 8500, Loss : 0.37384554743766785\n",
      "Epoch : 8600, Loss : 0.36715152859687805\n",
      "Epoch : 8700, Loss : 0.3688911199569702\n",
      "Epoch : 8800, Loss : 0.3684178292751312\n",
      "Epoch : 8900, Loss : 0.3675427734851837\n",
      "Epoch : 9000, Loss : 0.37624672055244446\n",
      "Epoch : 9100, Loss : 0.3710311949253082\n",
      "Epoch : 9200, Loss : 0.36896753311157227\n",
      "Epoch : 9300, Loss : 0.3728829026222229\n",
      "Epoch : 9400, Loss : 0.370792031288147\n",
      "Epoch : 9500, Loss : 0.37116310000419617\n",
      "Epoch : 9600, Loss : 0.3575183153152466\n",
      "Epoch : 9700, Loss : 0.35627099871635437\n",
      "Epoch : 9800, Loss : 0.36854565143585205\n",
      "Epoch : 9900, Loss : 0.36420994997024536\n",
      "Epoch : 10000, Loss : 0.36526980996131897\n",
      "torch.Size([644])\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    if i == 2 :\n",
    "        n_epochs = 10000\n",
    "    else :\n",
    "        n_epochs = 5000\n",
    "\n",
    "    model = training_loop(data = train_pyg[i], n_epochs = n_epochs)\n",
    "    # pred = predict(model, test_pyg[i])\n",
    "\n",
    "    z = model.encode(test_pyg[i].x, test_pyg[i].edge_index)\n",
    "    pred = torch.sigmoid(model.decode(z, test_pyg[i].edge_label_index))\n",
    "    print(pred.shape)\n",
    "    pred = pd.DataFrame(pred.cpu().detach().numpy())\n",
    "    pred = pd.concat([test_[i], pred], axis = 1)\n",
    "    pred = pred.drop(['to', 'from'], axis = 1)\n",
    "    pred.columns = ['id', 'prob']\n",
    "    pred.to_csv('./upload/pred_pyg_norm_leaky_{}.csv'.format(i + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, model):\n",
    "    z = model.encode(test_data.x, test_data.edge_index)\n",
    "    test_pred = torch.sigmoid(model.decode(z, test_data.edge_label_index))\n",
    "    return test_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
