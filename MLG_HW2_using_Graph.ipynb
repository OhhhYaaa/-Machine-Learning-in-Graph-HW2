{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLG HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rita/miniconda3/envs/jupyterlab/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, time, torch, json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch_geometric.utils import accuracy,sparse_mx_to_torch_sparse_tensor\n",
    "import torch_geometric.utils \n",
    "# from models.GCN import GCN\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm, trange\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# test \n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n",
      "(8686, 4)\n",
      "(4324, 2)\n",
      "torch.Size([3312, 3703])\n",
      "(7544, 4)\n",
      "(3736, 2)\n",
      "torch.Size([877, 1703])\n",
      "(2572, 4)\n",
      "(1273, 2)\n",
      "content :\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "test :\n",
      "         0     1     2\n",
      "0      id    to  from\n",
      "1  E10559  2323  2673\n",
      "train : \n",
      "        id    to  from  label\n",
      "0  E10311  2399  2339      0\n",
      "1  E10255  2397  1144      1\n",
      "edge_index : \n",
      "      to  from\n",
      "0  2397  1144\n",
      "1  2450  1312\n",
      "upload : \n",
      "        id  prob\n",
      "0  E10559   0.5\n",
      "1   E4849   0.5\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "test = []\n",
    "train = []\n",
    "upload = []\n",
    "edge_index = [] \n",
    "for i in range(3):\n",
    "    os.chdir('/home/rita/111/111-2MLG/HW2/dataset{}'.format(i + 1))\n",
    "    temp = pd.read_csv('./content.csv', header = None, sep = '\\t')\n",
    "    temp.sort_values(by = [0], inplace = True)\n",
    "    temp.set_index([0], inplace = True)\n",
    "    temp = torch.Tensor(np.array(temp)).to(torch.float32)\n",
    "    print(temp.shape)\n",
    "    content.append(temp)\n",
    "    test.append(pd.read_csv('./test.csv', header = None))\n",
    "    temp = pd.read_csv('./train.csv')\n",
    "    print(temp.shape)\n",
    "    train.append(temp)\n",
    "    temp = temp[temp.label == 1]\n",
    "    temp = temp[['to', 'from']]\n",
    "    temp = temp.reset_index(drop = True)\n",
    "    print(temp.shape)\n",
    "    edge_index.append(temp)\n",
    "    upload.append(pd.read_csv('./upload.csv'))\n",
    "print('content :\\n', content[0][:2])\n",
    "print('test :\\n', test[0].head(2))\n",
    "print('train : \\n', train[0].head(2))\n",
    "print('edge_index : \\n', edge_index[0].head(2))\n",
    "print('upload : \\n', upload[0].head(2))\n",
    "os.chdir('/home/rita/111/111-2MLG/HW2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# content preprocessing\n",
    "new_features = []\n",
    "for i in range(len(content)) :\n",
    "    t = content[i]\n",
    "    features_entropy = []\n",
    "    for i in range(t.shape[1]) :\n",
    "        temp = t.T[i]\n",
    "        t1 = torch.sum(temp == 0) / len(temp)\n",
    "        t2 = torch.sum(temp == 1) / len(temp)\n",
    "        temp = torch.tensor([t1, t2])\n",
    "        temp = entropy(temp)\n",
    "        if (temp == 0) :\n",
    "            temp = 0\n",
    "        else :\n",
    "            temp = 1 / temp\n",
    "        features_entropy.append(temp)\n",
    "    features_entropy = torch.tensor(features_entropy).reshape(1, -1)\n",
    "    t = t * features_entropy\n",
    "    t = t.type(torch.float32)\n",
    "    new_features.append(t)\n",
    "print(new_features[0][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\") #可以根據輸出結果知道是否有可用的GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ADJ(content, edge_index):\n",
    "    \n",
    "    s = content.shape[0]\n",
    "    m = np.zeros((s, s))\n",
    "    for i in range(edge_index.shape[0]):\n",
    "        x = edge_index.iat[i, 0]\n",
    "        y = edge_index.iat[i, 1]\n",
    "        m[x, y] = 1\n",
    "        m[y, x] = 1\n",
    "\n",
    "    return m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data1(dataset1):\n",
    "matrix =np.zeros( (2708, 2708) )\n",
    "#對應train資料有連接上則給予1，無則0\n",
    "for index,row in dataset1.iterrows():\n",
    "if row[3]==1:\n",
    "x = row[1]\n",
    "y = row[2]\n",
    "matrix[x,y] = 1\n",
    "else:\n",
    "x = row[1]\n",
    "y = row[2]\n",
    "matrix[x,y] = 0\n",
    "return matrix\n",
    "matrix1 = load_data1(dataset1)\n",
    "matrix1 = pd.DataFrame(matrix1)\n",
    "#因為有from跟to特徵所以在做一個to的adjacency matrix\n",
    "def load_data2(dataset1):\n",
    "matrix =np.zeros( (2708, 2708) )\n",
    "for index,row in dataset1.iterrows():\n",
    "if row[3]==1:\n",
    "x = row[1]\n",
    "y = row[2]\n",
    "matrix[y,x] = 1\n",
    "else:\n",
    "x = row[1]\n",
    "y = row[2]\n",
    "matrix[y,x] = 0\n",
    "return matrix\n",
    "#再將兩個matrix合併\n",
    "matrix2 = load_data2(dataset1)\n",
    "matrix2 = pd.DataFrame(matrix2)\n",
    "matrix = pd.concat([matrix1,matrix2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#再將content & adjacency matrix 合併成為最後想測試的資料特徵\n",
    "contentfinal = pd.concat([contentArray_sort,matrix],axis=1)\n",
    "contentfinal = contentfinal.values\n",
    "contentfinal = pd.DataFrame(contentfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#造to矩陣\n",
    "def load_data_to(dataset1):\n",
    "data = []\n",
    "for index, row in dataset1.iterrows():\n",
    "x = row[1]\n",
    "data.append(contentfinal.iloc[x,1:])\n",
    "return data\n",
    "df_to = load_data_to(dataset1)\n",
    "df_to = pd.DataFrame(df_to)\n",
    "df_to = df_to.values\n",
    "df_to = pd.DataFrame(df_to)\n",
    "造from矩陣\n",
    "def load_data_from(dataset1):\n",
    "data = []\n",
    "for index, row in dataset1.iterrows():\n",
    "x = row[2]\n",
    "data.append(contentfinal.iloc[x,1:])\n",
    "return data\n",
    "df_from = load_data_from(dataset1)\n",
    "df_from = pd.DataFrame(df_from)\n",
    "df_from = df_from.values\n",
    "df_from = pd.DataFrame(df_from)\n",
    "#將兩矩陣合併成測試資料\n",
    "df = pd.concat([df_to,df_from],axis=1)\n",
    "df = df.values\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:,:13698].values\n",
    "data = pd.DataFrame(data)\n",
    "label = dataset1.loc[:,'label'].values\n",
    "label = pd.DataFrame(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "encoding_dim = 100\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(13698,))\n",
    "# encoder layers\n",
    "encoded = Dense(1024, activation='relu')(input_img)\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(20, activation='relu')(encoded)\n",
    "encoder_output = Dense(encoding_dim)(encoded)\n",
    "# decoder layers\n",
    "decoded = Dense(20, activation='relu')(encoder_output)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(1024, activation='relu')(decoded)\n",
    "decoded = Dense(13698, activation='tanh')(decoded)\n",
    "# construct the autoencoder model\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "# construct the encoder model for plotting\n",
    "encoder = Model(input=input_img, output=encoder_output)\n",
    "# compile autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# training\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=20,\n",
    "                batch_size=256,\n",
    "                shuffle=True)\n",
    "# plotting\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "plt.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(512, activation='relu', kernel_initializer='random_normal', input_dim=13698))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(256, activation='relu', kernel_initializer='random_normal'))\n",
    "#Third  Hidden Layer\n",
    "classifier.add(Dense(64, activation='relu', kernel_initializer='random_normal'))\n",
    "#Forth  Hidden Layer\n",
    "classifier.add(Dense(20, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "#Compiling the neural network\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "classifier.fit(auto_x_train,label, batch_size=256, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test資料前處裡\n",
    "test1 = pd.read_csv(\"C:\\\\Users\\\\eagleuser\\\\Desktop\\\\hw1_data_update2\\\\dataset1\\\\test.csv\")\n",
    "test1.id = test1.id.str.replace('E',\"\")\n",
    "test1\n",
    "dd_to = load_data_to(test1)\n",
    "dd_to = pd.DataFrame(dd_to)\n",
    "dd_to = dd_to.values\n",
    "dd_to = pd.DataFrame(dd_to)\n",
    "dd_from = load_data_from(test1)\n",
    "dd_from = pd.DataFrame(dd_from)\n",
    "dd_from = dd_from.values\n",
    "dd_from = pd.DataFrame(dd_from)\n",
    "dd = pd.concat([dd_to,dd_from],axis=1)\n",
    "dd = dd.values\n",
    "dd = pd.DataFrame(dd)\n",
    "dd_test = pd.concat([dd,test1],axis=1)\n",
    "#放入autoencoder\n",
    "prob = encoder.predict(dd)\n",
    "auto_dd = autoencoder.predict(dd)\n",
    "auto_dd = pd.DataFrame(auto_dd)\n",
    "y_pred=classifier.predict(auto_dd)\n",
    "y_pred =(y_pred>0.5)\n",
    "y_pred\n",
    "data_test_label = classifier.predict_proba(auto_dd)\n",
    "data_test_label = pd.DataFrame(data_test_label)\n",
    "test1 = pd.read_csv(\"C:\\\\Users\\\\eagleuser\\\\Desktop\\\\hw1_data_update2\\\\dataset1\\\\test.csv\")\n",
    "result=pd.concat([test1,data_test_label],axis=1,join_axes=[test1.index])\n",
    "#存取測試資料\n",
    "result.to_csv('C:\\\\Users\\\\eagleuser\\\\Desktop\\\\hw1_data_update2\\\\resulut111.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_1 0.7592\n",
    "\n",
    "AP_1 0.7596\n",
    "\n",
    "AUC_2 0.7404\n",
    "\n",
    "AP_2 0.7720\n",
    "\n",
    "AUC_3 0.8866\n",
    "\n",
    "AP_3 0.9011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
